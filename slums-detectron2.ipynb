{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7140231,"sourceType":"datasetVersion","datasetId":4120181},{"sourceId":7147215,"sourceType":"datasetVersion","datasetId":4126059},{"sourceId":7147419,"sourceType":"datasetVersion","datasetId":4126224},{"sourceId":7150284,"sourceType":"datasetVersion","datasetId":4128309}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Detectron2 for Instance Segmentation of Individual Slum Units\n\n### Install detectron2 and import other dependencies\n\nAcknowledgement: much of this code is lifted and amended from [Detectron2's Colab Guide](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5). \n\nAll comments are my own.","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:26.691688Z","iopub.execute_input":"2023-12-08T02:42:26.692136Z","iopub.status.idle":"2023-12-08T02:42:47.882004Z","shell.execute_reply.started":"2023-12-08T02:42:26.692087Z","shell.execute_reply":"2023-12-08T02:42:47.880744Z"}}},{"cell_type":"code","source":"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import detectron2\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom detectron2.config import get_cfg\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.logger import setup_logger\nimport os, cv2, json, torch, random, shutil, glob\nfrom detectron2 import model_zoo\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.utils.visualizer import ColorMode, Visualizer\nfrom detectron2.engine import DefaultTrainer, DefaultPredictor, hooks\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader, MetadataCatalog, DatasetCatalog\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:47.884656Z","iopub.execute_input":"2023-12-08T02:42:47.885098Z","iopub.status.idle":"2023-12-08T02:42:50.379483Z","shell.execute_reply.started":"2023-12-08T02:42:47.885055Z","shell.execute_reply":"2023-12-08T02:42:50.378456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nThe detectron2 library requires its the input .json files to have a specific file structure, as indicated [here](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html).\n\nThe following code cells first copy the .json file of the slum dataset from input to working. The get_COCOFormat_data() function is used to process the .json file, which was obtained from makesense.ai (object detection, labeling, followed by exporting to COCO-formatted .json file). Note the line 'bbox_mode': BoxMode.XYWH_ABS. This means that the bounding box annotations are of the form (x1,y1,width,height). This is the correct form to use for makesense's output. Read [here](https://detectron2.readthedocs.io/en/latest/modules/structures.html?highlight=boxmode#detectron2.structures.BoxMode) for more details if your dataset is in another format.","metadata":{}},{"cell_type":"code","source":"# Copy the source .json for future manipulation\nsource_path_kd = '/kaggle/input/kibera-dharavi/kibera_dharavi/'\nsource_json_kd = 'kibera_dharavi_json.json'\ncopy_to_kd = '/kaggle/working/'\nsource_file = os.path.join(source_path_kd, source_json_kd)\ntarget_file = os.path.join(copy_to_kd, source_json_kd)\nshutil.copy(source_file, target_file)\nprint(f'Json file copied to {copy_to_kd}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:50.381777Z","iopub.execute_input":"2023-12-08T02:42:50.382828Z","iopub.status.idle":"2023-12-08T02:42:50.406101Z","shell.execute_reply.started":"2023-12-08T02:42:50.382787Z","shell.execute_reply":"2023-12-08T02:42:50.405164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_COCOFormat_data(json_file, path_to_images):\n    \n    with open(json_file, 'r') as file:\n        original_data = json.load(file)\n\n    image_info = {image['id']: {'file_name': image['file_name'], 'width': image['width'], 'height': image['height']} \n                  for image in original_data['images']}\n\n    new_data = {}\n    for annotation in tqdm(original_data['annotations']):\n        image_id = annotation['image_id']\n        if image_id in image_info:\n            bbox = annotation['bbox']\n            new_annotation = {\n                'bbox': bbox,\n                'bbox_mode': BoxMode.XYWH_ABS, #bbox of form (x1,y1,width,height)\n                'segmentation': annotation.get('segmentation', []),\n                'category_id': 0 #setting slum to class 0, detectron2 requires the max category id < num_classes\n            }\n\n            if image_id not in new_data:\n                filename = os.path.join(path_to_images, image_info[image_id]['file_name'])\n                new_data[image_id] = {\n                    'file_name': filename,\n                    'image_id': image_id,\n                    'height': image_info[image_id]['height'],\n                    'width': image_info[image_id]['width'],\n                    'annotations': [new_annotation]\n                }\n            else:\n                new_data[image_id]['annotations'].append(new_annotation)\n\n    \n    return list(new_data.values())\n\nkd_dict_data = get_COCOFormat_data('/kaggle/working/kibera_dharavi_json.json',\n                                   '/kaggle/input/kibera-dharavi/kibera_dharavi/kibera_dharavi_tiles')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:50.408120Z","iopub.execute_input":"2023-12-08T02:42:50.408434Z","iopub.status.idle":"2023-12-08T02:42:50.439797Z","shell.execute_reply.started":"2023-12-08T02:42:50.408407Z","shell.execute_reply":"2023-12-08T02:42:50.438824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iSAID Data\n\ndef get_iSAID_data():\n\n    # this .json file fixes an error in image 2678's (h,w) annotation\n    with open('/kaggle/input/isaid-coco/iSAID_COCO_with_fixed_val/iSAID_COCO_with_fixed_val/iSAID_val_fixed.json', 'r') as file:\n        original_data = json.load(file)\n    \n    image_info = {image['id']: {'file_name': image['file_name'], 'width': image['width'], 'height': image['height']} \n                  for image in original_data['images']}\n    \n    #do int(category['id'])-1 if you have no category id of 0\n    category_map = {category['id']: (int(category['id'])) for category in original_data['categories']}\n\n    new_data = {}\n    for annotation in tqdm(original_data['annotations']):\n        image_id = annotation['image_id']\n        if image_id in image_info:\n            bbox = annotation['bbox']\n            new_annotation = {\n                'bbox': bbox,\n                'bbox_mode': BoxMode.XYWH_ABS,\n                'segmentation': annotation.get('segmentation', []),\n                'category_id': category_map.get(annotation['category_id'], -1) #-1 for not found\n            }\n\n            if image_id not in new_data:\n                # where the real images actually are\n                filename = os.path.join('/kaggle/input/isaid-dota-validation-images/images', image_info[image_id]['file_name'])\n                new_data[image_id] = {\n                    'file_name': filename,\n                    'image_id': image_id,\n                    'height': image_info[image_id]['height'],\n                    'width': image_info[image_id]['width'],\n                    'annotations': [new_annotation]\n                }\n            else:\n                new_data[image_id]['annotations'].append(new_annotation)\n\n    \n    return list(new_data.values())\n\niSAID_dict_data = get_iSAID_data()","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:50.440981Z","iopub.execute_input":"2023-12-08T02:42:50.441274Z","iopub.status.idle":"2023-12-08T02:42:58.422147Z","shell.execute_reply.started":"2023-12-08T02:42:50.441248Z","shell.execute_reply":"2023-12-08T02:42:58.421093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting and registering the data\n\nThe following code cells split the data in training, validation, and test sets. The datasets are then registered. Note for registration: use DatasetCatalog.register(_train, lambda: get_data_dict(arg1, arg2))  \n","metadata":{}},{"cell_type":"code","source":"random.seed(10) #reproducability purposes\n\nimage_data = kd_dict_data + iSAID_dict_data\nprint(\"Merged dataset size:\", len(image_data))\nrandom.shuffle(image_data)\ntrain_size = len(image_data) // 2\nval_size = len(image_data) // 6\ntest_size = len(image_data) - train_size - val_size\n\ntrain_data = image_data[:train_size]\nval_data = image_data[train_size:train_size + val_size]\ntest_data = image_data[train_size + val_size:]\n\nprint('Data split.')\nprint(\"Train set size:\", len(train_data))\nprint(\"Validation set size:\", len(val_data))\nprint(\"Test set size:\", len(test_data))\n\n\n#These functions are needed to register your dataset with detectron2\ndef get_train_data():\n    return train_data\n\ndef get_val_data():\n    return val_data\n\ndef get_test_data():\n    return test_data\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:58.423562Z","iopub.execute_input":"2023-12-08T02:42:58.424013Z","iopub.status.idle":"2023-12-08T02:42:58.433322Z","shell.execute_reply.started":"2023-12-08T02:42:58.423975Z","shell.execute_reply":"2023-12-08T02:42:58.432321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# registering the datasets with detectron2\n\n# this placeholder is useful for re-running partial code, as you can not register the same name\n# for the dataset twice in a session.\nv = 6\n_train = f'train_slums_iSAID_v{v}'\n_val = f'val_slums_iSAID_v{v}'\n_test = f'test_slums_iSAID_v{v}'\n\n# You MUST pass a function which returns the correctly formatted data, read note above if your function takes arguments.\nDatasetCatalog.register(_train, get_train_data)\nDatasetCatalog.register(_val, get_val_data)\nDatasetCatalog.register(_test, get_test_data)\n\n# replace as needed\nclasses=['slum','storage_tank',\n'Large_Vehicle', 'Small_Vehicle',\n'ship', 'Harbor',\n'baseball_diamond', 'Ground_Track_Field',\n'Soccer_ball_field', 'Swimming_pool',\n'Roundabout', 'tennis_court',\n'basketball_court', 'plane',\n'Helicopter', 'Bridge']\n\n# needed for future visualizations\nMetadataCatalog.get(_train).set(thing_classes=classes)\nMetadataCatalog.get(_val).set(thing_classes=classes)\nMetadataCatalog.get(_test).set(thing_classes=classes)\n\ntrain_slums_iSAID_metadata = MetadataCatalog.get(_train)\nval_slums_iSAID_metadata = MetadataCatalog.get(_val)\ntest_slums_iSAID_metadata = MetadataCatalog.get(_test)\n\nprint('Datasets registered.')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:58.434564Z","iopub.execute_input":"2023-12-08T02:42:58.434918Z","iopub.status.idle":"2023-12-08T02:42:58.451086Z","shell.execute_reply.started":"2023-12-08T02:42:58.434826Z","shell.execute_reply":"2023-12-08T02:42:58.450040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Keep this around if you want to clear your directory for whatever reason.\n\n#shutil.rmtree(\"/kaggle/working/output\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:58.452228Z","iopub.execute_input":"2023-12-08T02:42:58.452516Z","iopub.status.idle":"2023-12-08T02:42:58.461149Z","shell.execute_reply.started":"2023-12-08T02:42:58.452492Z","shell.execute_reply":"2023-12-08T02:42:58.460219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training\n\nThis version of the notebook has code which would train the Cascade R-CNN, resuming from the 1000th iteration from my previous. training.cfg.MODEL.WEIGHTS is a useful, which is why the version","metadata":{}},{"cell_type":"code","source":"# Training MRCNN_R101_FPN_3x\n\nMRCNN_R101_FPN_3x = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\nMRCNN_X152 = \"Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml\"\n\n# Total iterations to run\ntotal_iterations = 20\niterations_per_loop = 10\nweight_path_152 = '/kaggle/input/152-1000-model-final/152_1000_model_final.pth'\n\nsetup_logger()\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MRCNN_X152)) #pre-trained model weights\ncfg.DATASETS.TRAIN = (_train,)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 4 # Faster than 2 or 8\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MRCNN_X152)  #weight_path_152\n#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.SOLVER.IMS_PER_BATCH = 1  # Kaggle's P100 can seem to handle only 1\ncfg.SOLVER.BASE_LR = 0.00025  # set small due to fine-tuning, increase if randomly intialized model or adjust accordingly.\ncfg.SOLVER.MAX_ITER = total_iterations   \ncfg.SOLVER.STEPS = []        \ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 16\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n\n#Training loop to mitigate chances of a Kaggle-crash. Set resume=False to train the model just the one time, and take it out of the loop\nfor iteration in range(0, total_iterations, iterations_per_loop):\n    trainer = DefaultTrainer(cfg)\n    cfg.SOLVER.MAX_ITER = iteration + iterations_per_loop\n    trainer.resume_or_load(resume = iteration > 0)\n    trainer.train()\n\n# After training, set the weights for inference\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:42:58.462348Z","iopub.execute_input":"2023-12-08T02:42:58.462937Z","iopub.status.idle":"2023-12-08T02:43:27.495086Z","shell.execute_reply.started":"2023-12-08T02:42:58.462901Z","shell.execute_reply":"2023-12-08T02:43:27.493228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nThe following code cells return the COCO evaluation of the model on the test set, and also visualize an image and its corresponding predicted labels. If you've just trained your model a little bit, and loss is still high, you might get a no predictions error. Consider training for longer or decreasing the cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 to display annotations which have a low likelihood of being correct.","metadata":{}},{"cell_type":"code","source":"#cfg.MODEL.WEIGHTS = '#os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") #feel free to change this\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  \npredictor = DefaultPredictor(cfg)\ntest_loader = build_detection_test_loader(cfg, _test)\nevaluator = COCOEvaluator(_test, output_dir=\"./output\")\nprint(inference_on_dataset(predictor.model, test_loader, evaluator))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:49:46.832802Z","iopub.execute_input":"2023-12-08T02:49:46.833493Z","iopub.status.idle":"2023-12-08T02:53:38.822944Z","shell.execute_reply.started":"2023-12-08T02:49:46.833455Z","shell.execute_reply":"2023-12-08T02:53:38.821895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") \ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n\npredictor = DefaultPredictor(cfg)\n\nim = cv2.imread(\"/kaggle/input/kibera-dharavi/kibera_dharavi/kibera_dharavi_tiles/dharavi_tile_5_2.jpg\")\noutputs = predictor(im)  \n\nv = Visualizer(im[:, :, ::-1],\n               metadata=test_slums_iSAID_metadata, \n               scale=0.5, \n               instance_mode=ColorMode.IMAGE_BW)\n\n# Output and save prediction\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.imshow(out.get_image()[:, :, ::-1])\noutput_path = '/kaggle/working/output_image.jpg'\nplt.savefig(output_path, bbox_inches='tight', pad_inches=0)\nplt.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T03:08:31.368845Z","iopub.execute_input":"2023-12-08T03:08:31.369837Z","iopub.status.idle":"2023-12-08T03:08:36.527202Z","shell.execute_reply.started":"2023-12-08T03:08:31.369802Z","shell.execute_reply":"2023-12-08T03:08:36.526365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Please email me at raphael@uni.minerva.edu if you have any questions","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:43:27.499468Z","iopub.status.idle":"2023-12-08T02:43:27.499839Z","shell.execute_reply.started":"2023-12-08T02:43:27.499648Z","shell.execute_reply":"2023-12-08T02:43:27.499665Z"},"trusted":true},"execution_count":null,"outputs":[]}]}